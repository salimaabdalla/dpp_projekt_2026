{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caeb0287",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import moduls\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "from math import nan\n",
    "\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from imblearn.pipeline import Pipeline \n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.svm import LinearSVR\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.svm import SVR\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import Dropout\n",
    "import keras.losses as kl\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "from core.classes import FeatureEngineer, DataImputation\n",
    "\n",
    "pd.set_option('display.max_columns', None)\n",
    "plt.style.use('bmh')\n",
    "\n",
    "import statsmodels\n",
    "from statsmodels.tsa.statespace.sarimax import SARIMAX\n",
    "from statsmodels.tsa.stattools import adfuller\n",
    "from statsmodels.tsa.stattools import kpss\n",
    "from statsmodels.graphics.tsaplots import plot_acf, plot_pacf\n",
    "from statsmodels.tsa.seasonal import seasonal_decompose\n",
    "\n",
    "\n",
    "import warnings\n",
    "# skforecast\n",
    "import skforecast\n",
    "from skforecast.datasets import fetch_dataset\n",
    "from skforecast.plot import set_dark_theme, plot_prediction_intervals\n",
    "from skforecast.stats import Sarimax, Arima\n",
    "from skforecast.recursive import ForecasterStats\n",
    "from skforecast.utils import expand_index\n",
    "from skforecast.model_selection import TimeSeriesFold, backtesting_stats, grid_search_stats\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac7ce3f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '../data/processed/combined_data_with_elasticity.csv'\n",
    "df = pd.read_csv(path, index_col='date', skipfooter=1, parse_dates=True)\n",
    "size_diagrams = (10,3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae7f57fa",
   "metadata": {},
   "source": [
    "## Classes for training the models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ceaf826",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(TransformerMixin, BaseEstimator):\n",
    "    \"\"\"Imputs missing values. \"\"\"\n",
    "    \n",
    "    def __init__(self,columns, model_start, model_middle, model_end): \n",
    "        \"\"\"Initalizes the attributes.\"\"\"\n",
    "        self.features = []\n",
    "        self.num_cols = []\n",
    "        self.dict_num = {}\n",
    "        self.model_start = model_start\n",
    "        self.model_middle = model_middle \n",
    "        self.model_end = model_end\n",
    "        self.columns = columns\n",
    "        self.weight_mond = 0\n",
    "        self.weight_fer = 0\n",
    "        self.weight_mond_fer = 0\n",
    "        self.weight_fer_fer = 0\n",
    "        self.weight_lindt = 0\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        mask_f = X.loc[:,'Ferrero'].isna()\n",
    "        mask_l = X.loc[:,'LindtSpruengli'].isna()\n",
    "        number_mond = (~X['Mondelez'].isna()).sum()\n",
    "        number_fer = (~X['Ferrero'].isna()).sum()\n",
    "        number_lindt = (~X['LindtSpruengli'].isna()).sum()\n",
    "        number_all = number_mond + number_fer + number_lindt\n",
    "        self.weight_mond_fer = number_mond / (number_mond + number_fer)\n",
    "        self.weight_fer_fer = number_fer / (number_mond + number_fer)\n",
    "        self.weight_mond = number_mond / number_all\n",
    "        self.weight_fer = number_fer / number_all\n",
    "        self.weight_lindt = number_lindt / number_all\n",
    "        feat_start = X\n",
    "        target_start = y\n",
    "        self.model_start.fit(feat_start.loc[:,self.columns + ['Mondelez']], target_start)\n",
    "\n",
    "        feat_middle = X.copy()\n",
    "        feat_middle = feat_middle.loc[~mask_f,:]\n",
    "        target_middle = y.loc[~mask_f]\n",
    "        feat_middle['target_pred_by_1'] = self.model_start.predict(feat_middle.loc[:,self.columns + ['Mondelez']])\n",
    "\n",
    "        self.model_middle.fit(feat_middle.loc[:,['target_pred_by_1', 'Ferrero']], target_middle)\n",
    "\n",
    "        feat_end = X.copy()\n",
    "        feat_end = feat_end.loc[~mask_f & ~mask_l,:]\n",
    "        target_end = y.loc[~mask_f & ~mask_l]\n",
    "        feat_end['target_pred_by_1'] = self.model_start.predict(feat_end.loc[:,self.columns + ['Mondelez']])\n",
    "        feat_end['target_pred_by_2'] = self.model_middle.predict(feat_end.loc[:,['target_pred_by_1', 'Ferrero']])\n",
    "\n",
    "        self.model_end.fit(feat_end.loc[:,['target_pred_by_2', 'LindtSpruengli']], target_end)\n",
    "\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X, y=None):\n",
    "        \"\"\"Fill the missing values and drop some columns.\n",
    "        \n",
    "        \n",
    "        \"\"\"\n",
    "        df = X.copy()\n",
    "\n",
    "        self.features = df.columns.values\n",
    "        return df\n",
    "    \n",
    "    def predict(self, X, y=None):\n",
    "        \"\"\"Fill the missing values and drop some columns.\n",
    "        \n",
    "        \n",
    "        \"\"\"\n",
    "        mask_f = X.loc[:,'Ferrero'].isna()\n",
    "        mask_l = X.loc[:,'LindtSpruengli'].isna()\n",
    "        result = pd.DataFrame.from_dict({'index': X.index.values})\n",
    "        result = result.set_index('index')\n",
    "        result['pred_m'] = nan\n",
    "        result['pred_mf'] = nan\n",
    "        result['pred_mfl'] = nan\n",
    "        feat_middle = X.copy()\n",
    "        feat_middle = feat_middle.loc[~mask_f,:]\n",
    "        feat_middle['target_pred_by_1'] = self.model_start.predict(feat_middle.loc[:,self.columns + ['Mondelez']])\n",
    "        result.loc[:,'pred_m'] = self.model_start.predict(X.loc[:,self.columns + ['Mondelez']])\n",
    "       \n",
    "        feat_end = X.copy()\n",
    "        feat_end = feat_end.loc[~mask_f & ~mask_l,:]\n",
    "        feat_end['target_pred_by_1'] = self.model_start.predict(feat_end.loc[:,self.columns + ['Mondelez']])\n",
    "        feat_end['target_pred_by_2'] = self.model_middle.predict(feat_end.loc[:,['target_pred_by_1', 'Ferrero']])\n",
    "        result.loc[~mask_f,'pred_mf'] = self.model_middle.predict(feat_middle.loc[:,['target_pred_by_1', 'Ferrero']])\n",
    "        result.loc[~mask_f & ~mask_l,'pred_mfl'] = self.model_end.predict(feat_end.loc[:,['target_pred_by_2', 'LindtSpruengli']])\n",
    "        result['final'] = 0\n",
    "        result.loc[mask_f, 'final'] = result.loc[mask_f,'pred_m'] \n",
    "        result.loc[~mask_f & mask_l,'final'] = result.loc[~mask_f & mask_l,'pred_m'] * self.weight_mond_fer + result.loc[~mask_f & mask_l,'pred_mf'] * self.weight_fer_fer \n",
    "        result.loc[~mask_l,'final'] = result.loc[~mask_l,'pred_m'] * self.weight_mond + result.loc[~mask_l,'pred_mf'] * self.weight_fer + result.loc[~mask_l,'pred_mfl'] * self.weight_lindt\n",
    "        return result\n",
    "    \n",
    "    def get_feature_names_out(self, input_features=None):\n",
    "        \"\"\"Returns the names of the features.\"\"\"\n",
    "        return self.features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acc1d0b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model_pred_error(TransformerMixin, BaseEstimator):\n",
    "    \"\"\"Imputs missing values. \"\"\"\n",
    "    \n",
    "    def __init__(self, columns, model_start, model_middle, model_end): \n",
    "        \"\"\"Initalizes the attributes.\"\"\"\n",
    "        self.features = []\n",
    "        self.num_cols = []\n",
    "        self.dict_num = {}\n",
    "        self.model_start = model_start\n",
    "        self.model_middle = model_middle \n",
    "        self.model_end = model_end\n",
    "        self.columns = columns\n",
    "        self.weight_mond = 0\n",
    "        self.weight_fer = 0\n",
    "        self.weight_mond_fer = 0\n",
    "        self.weight_fer_fer = 0\n",
    "        self.weight_lindt = 0\n",
    "        self.eta = 0.9\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        mask_f = X.loc[:,'Ferrero'].isna()\n",
    "        mask_l = X.loc[:,'LindtSpruengli'].isna()\n",
    "        number_mond = (~X['Mondelez'].isna()).sum()\n",
    "        number_fer = (~X['Ferrero'].isna()).sum()\n",
    "        number_lindt = (~X['LindtSpruengli'].isna()).sum()\n",
    "        number_all = number_mond + number_fer + number_lindt\n",
    "        self.weight_mond_fer = number_mond / (number_mond + number_fer)\n",
    "        self.weight_fer_fer = number_fer / (number_mond + number_fer)\n",
    "        self.weight_mond = number_mond / number_all\n",
    "        self.weight_fer = number_fer / number_all\n",
    "        self.weight_lindt = number_lindt / number_all\n",
    "        feat_start = X\n",
    "        target_start = y\n",
    "        self.model_start.fit(feat_start.loc[:,self.columns + ['Mondelez']], target_start)\n",
    "        y_start_predicted = self.model_start.predict(feat_start.loc[:,self.columns + ['Mondelez']])\n",
    "        \n",
    "        feat_middle = X.copy()\n",
    "        target_middle = (y-self.eta*y_start_predicted).loc[~mask_f]\n",
    "        \n",
    "        self.model_middle.fit(feat_middle.loc[~mask_f,self.columns + ['Mondelez', 'Ferrero']], target_middle)\n",
    "        y_middle_predicted = self.model_middle.predict(feat_middle.loc[~mask_f & ~mask_l,self.columns + ['Mondelez', 'Ferrero']])\n",
    "        feat_end = X.copy()\n",
    "        feat_end = feat_end.loc[~mask_f & ~mask_l,:]\n",
    "        target_end =( y-self.eta*y_start_predicted).loc[~mask_f & ~mask_l] \n",
    "        target_end = target_end - self.eta**2 * y_middle_predicted\n",
    "\n",
    "        self.model_end.fit(feat_end.loc[:,self.columns + ['Mondelez', 'Ferrero', 'LindtSpruengli']], target_end)\n",
    "\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X, y=None):\n",
    "        \"\"\"Fill the missing values and drop some columns.\n",
    "        \n",
    "        \n",
    "        \"\"\"\n",
    "        df = X.copy()\n",
    "\n",
    "        self.features = df.columns.values\n",
    "        return df\n",
    "    \n",
    "    def predict(self, X, y=None):\n",
    "        \"\"\"Fill the missing values and drop some columns.\n",
    "        \n",
    "        \n",
    "        \"\"\"\n",
    "        mask_f = X.loc[:,'Ferrero'].isna()\n",
    "        mask_l = X.loc[:,'LindtSpruengli'].isna()\n",
    "\n",
    "        result = pd.DataFrame.from_dict({'index': X.index.values})\n",
    "        result = result.set_index('index')\n",
    "        result['pred_m'] = nan\n",
    "        result['pred_mf'] = nan\n",
    "        result['pred_mfl'] = nan\n",
    "        feat_middle = X.copy()\n",
    "\n",
    "        result['pred_m'] = self.model_start.predict(feat_middle.loc[:,self.columns + ['Mondelez']])\n",
    "\n",
    "        result.loc[~mask_f ,'pred_mf'] = self.model_middle.predict(feat_middle.loc[~mask_f, self.columns + ['Mondelez', 'Ferrero']])\n",
    "        result.loc[~mask_f & ~mask_l ,'pred_mfl'] = self.model_end.predict(feat_middle.loc[~mask_f & ~mask_l,  self.columns + ['Mondelez', 'Ferrero', 'LindtSpruengli']])\n",
    "        result['final'] = 0\n",
    "        result.loc[mask_f,'final'] = result.loc[mask_f,'pred_m'] \n",
    "        result.loc[~mask_f & mask_l,'final'] = result.loc[~mask_f & mask_l,'pred_m'] * self.eta + result.loc[~mask_f & mask_l,'pred_mf'] \n",
    "        result.loc[~mask_l,'final'] = result.loc[~mask_l,'pred_m'] * self.eta + result.loc[~mask_l,'pred_mf'] * self.eta**2 + result.loc[~mask_l,'pred_mfl']\n",
    "        return result\n",
    "    \n",
    "    def get_feature_names_out(self, input_features=None):\n",
    "        \"\"\"Returns the names of the features.\"\"\"\n",
    "        return self.features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "195aacac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing_split(df, dict_lag, number_train, target_name):\n",
    "    shares = ['Ferrero', 'LindtSpruengli', 'Mondelez']\n",
    "    columns = [x for x in list(dict_lag) if x not in shares]\n",
    "    col_with_target = columns\n",
    "    if target_name not in columns: \n",
    "        col_with_target.append(target_name)\n",
    "    df_selected_col = df.loc[:,shares + col_with_target]\n",
    "    df_selected_col = df_selected_col.dropna(subset=target_name)\n",
    "    df_train = df_selected_col.iloc[:number_train,:]\n",
    "    df_test = df_selected_col.iloc[number_train:,:]\n",
    "    target_train = df_train[target_name]\n",
    "    target_test = df_test[target_name]\n",
    "    features_train = df_train.loc[:,shares + columns]\n",
    "    features_test = df_test.loc[:,shares + columns]\n",
    "    return shares, columns, target_train, target_test, features_train, features_test \n",
    "\n",
    "def preprocessing(df, dict_lag, number_train, target_name):\n",
    "    shares, columns, target_train, target_test, features_train, features_test  = preprocessing_split(df, dict_lag, number_train, target_name)\n",
    "    ohe_tranf = ColumnTransformer(transformers = [('ohe', OneHotEncoder(sparse_output=False, handle_unknown='ignore'), \n",
    "                                                  ['month'])], \n",
    "                                  remainder = 'passthrough', verbose_feature_names_out = False)\n",
    "    pipeline = Pipeline([('DaIm', DataImputation(shares)), \n",
    "                               ('FE', FeatureEngineer(dict_lag)),\n",
    "                               ('ohe', ohe_tranf),\n",
    "                               ('sca', StandardScaler())])\n",
    "    \n",
    "    features_train_transf = pipeline.fit_transform(features_train)\n",
    "    features_test_transf = pipeline.transform(features_test)\n",
    "    # make the features into a dataframe again\n",
    "    features_train_transf = pd.DataFrame(columns=pipeline.get_feature_names_out(), data=features_train_transf)\n",
    "    features_train_transf.index = features_train.index\n",
    "    features_test_transf = pd.DataFrame(columns=pipeline.get_feature_names_out(), data=features_test_transf)\n",
    "    features_test_transf.index = features_test.index\n",
    "    columns = [x for x in pipeline.get_feature_names_out() if x not in shares]\n",
    "    return features_train_transf, features_test_transf, target_train, target_test, columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "387d2985",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model_combination(df, dict_lag, model_start, model_middle, model_end, number_train, target_name, name='Model', model_simple=0):\n",
    "    # Train Test split\n",
    "    shares, columns, target_train, target_test, features_train, features_test = preprocessing_split(df, dict_lag, number_train, target_name)\n",
    "    ohe_tranf = ColumnTransformer(transformers = [('ohe', OneHotEncoder(sparse_output=False, handle_unknown='ignore'), \n",
    "                                                  ['month'])], \n",
    "                                  remainder = 'passthrough', verbose_feature_names_out=False)\n",
    "    pipeline = Pipeline([('DaIm', DataImputation(shares)), \n",
    "                               ('FE', FeatureEngineer(dict_lag)),\n",
    "                               ('ohe', ohe_tranf),\n",
    "                               ('sca', StandardScaler())])\n",
    "    \n",
    "    features_train_transf = pipeline.fit_transform(features_train)\n",
    "    features_test_transf = pipeline.transform(features_test)\n",
    "    # make the features into a dataframe again\n",
    "    features_train_transf = pd.DataFrame(columns=pipeline.get_feature_names_out(), data=features_train_transf)\n",
    "    features_train_transf.index = features_train.index\n",
    "    features_test_transf = pd.DataFrame(columns=pipeline.get_feature_names_out(), data=features_test_transf)\n",
    "    features_test_transf.index = features_test.index\n",
    "    columns = [x for x in pipeline.get_feature_names_out() if x not in shares]\n",
    "    # Model\n",
    "    if model_simple == 0:\n",
    "        mod = Model( columns, model_start, \n",
    "                    model_middle, model_end)\n",
    "    else: \n",
    "        mod = Model_pred_error(columns, model_start, \n",
    "                   model_middle, model_end) \n",
    "    mod.fit(features_train_transf, target_train)\n",
    "\n",
    "    #Predict Train data\n",
    "    results = mod.predict(features_train_transf, target_train)\n",
    "    mse_train = mean_squared_error(results['final'], target_train)\n",
    "    print('MSE on Train set: ', mse_train)\n",
    "    results['target'] = target_train\n",
    "    last_train_df = results.iloc[-1,:]\n",
    "    last_train_df = pd.DataFrame(last_train_df).transpose()\n",
    "    results_test = mod.predict(features_test_transf, target_test)\n",
    "    results_test['target'] = target_test\n",
    "    mse_test = mean_squared_error(results_test['final'], target_test)\n",
    "    print('MSE on Test set: ', mse_test)\n",
    "    results_test = pd.concat([last_train_df, results_test])\n",
    "    sns.set(font_scale=1)\n",
    "    color_target = 'darkred'\n",
    "    color_final = 'darkgreen'\n",
    "    color_m = 'orchid'\n",
    "    color_mf = 'lightblue'\n",
    "    color_mfl = 'pink'\n",
    "    fig, ax = plt.subplots(nrows=1, ncols=1, figsize=size_diagrams)\n",
    "    ax.plot(results.index, results.pred_m.values, label='Hilfsmodell 1', alpha=0.5, color=color_m)\n",
    "    ax.plot(results.index, results.pred_mf.values, label='Hilfsmodell 2', alpha=0.5, color=color_mf)\n",
    "    ax.plot(results.index, results.pred_mfl.values, label='Hilfsmodell 3', alpha=0.5, color=color_mfl)\n",
    "    ax.plot(results.index, results.final.values, label='Finales Modell', color=color_final)\n",
    "    ax.plot(results.index, results.target, label='Target', color=color_target);\n",
    "    ax.set_ylabel('Price elasticity', alpha=0.8)\n",
    "    ax.set_title(name)\n",
    "    \n",
    "    # Predict testdata \n",
    "    ax.plot(results_test.index, results_test.target, color=color_target, linestyle='--')\n",
    "    \n",
    "    ax.plot(results_test.index, results_test.pred_m.values, alpha=0.5, linestyle='--', color=color_m)\n",
    "    ax.plot(results_test.index, results_test.pred_mf.values, alpha=0.5, linestyle='--', color=color_mf)\n",
    "    ax.plot(results_test.index, results_test.pred_mfl.values, alpha=0.5, linestyle='--', color=color_mfl)\n",
    "    ax.plot(results_test.index, results_test.final.values,  linestyle='--', color=color_final)\n",
    "    xlim = ax.get_xlim()\n",
    "    ylim = ax.get_ylim()\n",
    "    ylim_min = ax.get_ylim()[0]\n",
    "    ylim_max = ax.get_ylim()[1]\n",
    "    laenge = ylim_max - ylim_min \n",
    "    laenge_neu = laenge * 0.9 \n",
    "    laenge_neu2 = laenge_neu * 0.9\n",
    "    xmax = ax.get_xlim()[1]\n",
    "    ax.fill_betweenx(ylim, pd.to_datetime('2021-04-30'), xmax, alpha=0.1, color='black')\n",
    "    ax.fill_betweenx([laenge_neu * ylim_min / laenge , laenge_neu * ylim_max / laenge], pd.to_datetime('2024-11-30'), xmax, alpha=0.1, color='black')\n",
    "    ax.fill_betweenx([laenge_neu2 * ylim_min / laenge , laenge_neu2 * ylim_max / laenge], pd.to_datetime('2025-02-28'), xmax, alpha=0.1, color='black', hatch='xx', label='Train data')\n",
    "    ax.text(x=pd.to_datetime('2019-02-05'), y=ylim[0], s='Mondelez', fontsize='x-large', alpha=0.2)\n",
    "    ax.text(x=pd.to_datetime('2021-05-05'), y=ylim[0], s='Ferrero', fontsize='x-large', alpha=0.2)\n",
    "    ax.text(x=pd.to_datetime('2024-12-02'), y=laenge_neu * ylim_min / laenge, s='Lindt', fontsize='medium', alpha=0.2)\n",
    "    ax.set_xlim([pd.to_datetime('2019-01-01'), pd.to_datetime('2026-01-01') ])\n",
    "    ax.legend()\n",
    "    return results, results_test, mse_train, mse_test, mod, pipeline\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5353d96",
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_importance(model, pipeline, df, dict_lag, number_train, target_name, predict_typ='normal'):\n",
    "    \"\"\"Calculate the importance of so features from the dataset and the engineered features and plot them.\n",
    "    \n",
    "    Args:\n",
    "        pipeline (pipeline): Pipeline with the following steps as feature engineering: 'daTr', 'daIm', 'FE', 'm_daIm'\n",
    "                             and afterwards the following steps doing the predictions: 'preprocessor', 'model'\n",
    "    \n",
    "    Returns:\n",
    "        dict: Feature importance of the original features, given as the values of the dict\n",
    "        dict: Feature importance of the engineered features, given as the values of the dict\n",
    "    \n",
    "    \"\"\"\n",
    "    shares, columns, target_train, target_test, features_train, features_test = preprocessing_split(df, dict_lag, number_train, target_name)\n",
    "    features_train_transf = pipeline.transform(features_train)\n",
    "    features_test_transf = pipeline.transform(features_test)\n",
    "    # make the features into a dataframe again\n",
    "    features_train_transf = pd.DataFrame(columns=pipeline.get_feature_names_out(), data=features_train_transf)\n",
    "    features_train_transf.index = features_train.index\n",
    "    features_test_transf = pd.DataFrame(columns=pipeline.get_feature_names_out(), data=features_test_transf)\n",
    "    features_test_transf.index = features_test.index\n",
    "    columns = [x for x in pipeline.get_feature_names_out() if x not in shares]\n",
    "    if predict_typ == 'stat': \n",
    "        scor_orig = mean_squared_error(target_test.values.tolist(), model_res.get_forecast(steps=len(target_test), exog = features_test_transf).predicted_mean)\n",
    "    elif predict_typ == 'normal':\n",
    "        scor_orig = mean_squared_error(target_test.values.tolist(), model.predict(features_test_transf).final)\n",
    "    else:\n",
    "        scor_orig = mean_squared_error(target_test.values.tolist(), model.predict(features_test_transf))\n",
    "    \n",
    "    features_test_perm = features_test.copy()\n",
    "    columns = features_test_perm.columns.values\n",
    "    for i in range(2, len(columns)):\n",
    "        col = columns[i]\n",
    "        try:\n",
    "            age_series_perm = features_test_perm[col].sample(frac=1, replace=False, random_state=i+1)\n",
    "            age_series_perm = age_series_perm.reset_index(drop=True).tolist()      \n",
    "            features_test_perm[col] = age_series_perm\n",
    "        except: \n",
    "            print('Spalte nicht vorhanden')\n",
    "    features_test_transf_perm = pipeline.transform(features_test_perm)\n",
    "    features_test_transf_perm = pd.DataFrame(columns=pipeline.get_feature_names_out(), data=features_test_transf_perm)\n",
    "    features_test_transf_perm.index = features_test.index\n",
    "    if predict_typ == 'stat': \n",
    "        vgl = mean_squared_error(target_test.values.tolist(), model_res.get_forecast(steps=len(target_test), exog = features_test_transf_perm).predicted_mean)\n",
    "    elif predict_typ == 'normal':\n",
    "        vgl = mean_squared_error(target_test.values.tolist(), model.predict(features_test_transf_perm).final)\n",
    "    else:\n",
    "        vgl = mean_squared_error(target_test.values.tolist(), model.predict(features_test_transf_perm))\n",
    "  \n",
    "    features_test_perm = features_test.copy()\n",
    "    columns = features_test_perm.columns.values\n",
    "    results_FI_start = pd.DataFrame([np.full(len(['Model'] + list(dict_lag.keys())), float('nan'))], columns=['Model'] + list(dict_lag.keys()))  \n",
    "    for i in range(0, len(columns)):\n",
    "        col = columns[i]\n",
    "        features_test_perm = features_test.copy()\n",
    "        age_series_perm = features_test_perm[col].sample(frac=1, replace=False, random_state=i+1)\n",
    "        age_series_perm = age_series_perm.reset_index(drop=True).tolist()    \n",
    "        features_test_perm[col] = age_series_perm\n",
    "        features_test_transf_perm = pipeline.transform(features_test_perm)\n",
    "        features_test_transf_perm = pd.DataFrame(columns=pipeline.get_feature_names_out(), data=features_test_transf_perm)\n",
    "        features_test_transf_perm.index = features_test.index\n",
    "        if predict_typ == 'stat': \n",
    "            score = mean_squared_error(target_test.values.tolist(), model_res.get_forecast(steps=len(target_test), exog = features_test_transf_perm).predicted_mean)\n",
    "        elif predict_typ == 'normal':\n",
    "            score = mean_squared_error(target_test.values.tolist(), model.predict(features_test_transf_perm).final)\n",
    "        else:\n",
    "            score = mean_squared_error(target_test.values.tolist(), model.predict(features_test_transf_perm))\n",
    "        results_FI_start.loc[0,col] = score-scor_orig\n",
    "\n",
    "    columns_FE = features_test_transf.columns.values.tolist()\n",
    "    results_FI_end = pd.DataFrame([np.full(len(['Model'] + columns_FE), float('nan'))], columns=['Model'] + columns_FE)\n",
    "    for i in range(len(columns_FE)):\n",
    "        col = columns_FE[i]\n",
    "        # Copy data\n",
    "        features_test_perm = features_test_transf.copy()\n",
    "        # Shuffle one column\n",
    "        age_series_perm = features_test_perm[col].sample(frac=1, replace=False, random_state=0)\n",
    "        age_series_perm = age_series_perm.reset_index(drop=True).tolist()\n",
    "        features_test_perm[col] = age_series_perm\n",
    "        if predict_typ == 'stat': \n",
    "            score = mean_squared_error(target_test.values.tolist(), model_res.get_forecast(steps=len(target_test), exog = features_test_perm).predicted_mean)\n",
    "        elif predict_typ == 'normal':\n",
    "            score = mean_squared_error(target_test.values.tolist(), model.predict(features_test_perm).final)\n",
    "        else:\n",
    "            score = mean_squared_error(target_test.values.tolist(), model.predict(features_test_perm))\n",
    "        results_FI_end.loc[0,col] = score-scor_orig\n",
    "    return results_FI_start, results_FI_end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2567d8d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(name, number_train=74, incl_shares=True):\n",
    "    if incl_shares == True:\n",
    "        shares = ['Ferrero', 'LindtSpruengli', 'Mondelez']\n",
    "    else:\n",
    "        shares = []\n",
    "\n",
    "    if name == 'coffee':\n",
    "        target_name = 'elasticity_coffee'\n",
    "        dict_lag = {}\n",
    "        for k in shares + ['month']:\n",
    "            dict_lag.update({k: [0]})\n",
    "        for k in [\n",
    "            'EinfPr_Kaffee und Tee, Kaffee-Ersatz',\n",
    "            'VPI_Kaffee und Ähnliches', \n",
    "            'elasticity_coffee']:\n",
    "            dict_lag.update({k: [1]})\n",
    "\n",
    "        for k in ['ErzPr_Kaffee und Tee, Kaffee-Ersatz', \n",
    "                    'PCOCOUSDM',\n",
    "                        'PCOFFROBUSDM']:\n",
    "            dict_lag.update({k: [0,1]})\n",
    "    elif name == 'cacao':\n",
    "        columns = [\n",
    "            #'ErzPr_Schokolade u.a. kakaoh. Lebensm.zub.,in Verp.>2kg',\n",
    "            #'ErzPr_Schokolade u.a. kakaoh. Leb.m.zuber.,in Verp.<=2kg',\n",
    "            #'ErzPr_Süßwaren oh. Kakaogeh. (einschl.weißer Schokolade)',\n",
    "            'month',\n",
    "            'ErzPr_Schokolade u.a. kakaoh. Leb.m.zuber.,in Verp.<=2kg',\n",
    "            'EinfPr_Süßwaren (ohne Dauerbackwaren)',\n",
    "            'VPI_Schokoladen', \n",
    "            #'VPI_Süßwaren',\n",
    "            #'VPI_Kakaopulver oder Ähnliches',\n",
    "            'PCOCOUSDM',\n",
    "            'PCOFFROBUSDM']\n",
    "        target_name = 'elasticity_cacao'\n",
    "        dict_lag = {}\n",
    "        for k in shares:\n",
    "            dict_lag.update({k: [0]})\n",
    "            dict_lag.update({'Mondelez': [0,1,2,3]})\n",
    "        for k in [ #'Umsatz_WZ08-1082',\n",
    "            #'Wert der zum Absatz bestimmten Produktion_Schokolade u.a. kakaohaltige Lebensmittelzubereit.',\n",
    "            'EinfPr_Süßwaren (ohne Dauerbackwaren)',\n",
    "            'VPI_Schokoladen',\n",
    "            #'VPI_Süßwaren',\n",
    "            #'VPI_Kakaopulver oder Ähnliches',\n",
    "            'PCOCOUSDM',\n",
    "            'PCOFFROBUSDM', \n",
    "            'elasticity_cacao']:\n",
    "            dict_lag.update({k: [1]})\n",
    "\n",
    "        for k in [\n",
    "            #'ErzPr_Schokolade u.a. kakaoh. Lebensm.zub.,in Verp.>2kg',\n",
    "            #'ErzPr_Schokolade u.a. kakaoh. Leb.m.zuber.,in Verp.<=2kg',\n",
    "            #'ErzPr_Süßwaren oh. Kakaogeh. (einschl.weißer Schokolade)',\n",
    "            'month',\n",
    "            'ErzPr_Schokolade u.a. kakaoh. Leb.m.zuber.,in Verp.<=2kg', \n",
    "            'PCOCOUSDM',\n",
    "            'PCOFFROBUSDM']:\n",
    "            dict_lag.update({k: [0,1]})\n",
    "    else: \n",
    "        print('Only \"coffee\" and \"cacao\" are valid entries.')\n",
    "        return -1\n",
    "    return target_name, number_train, shares, dict_lag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af48d6df",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_both_models(name_start, df, dict_lag, model_start, model_middle, model_end, number_train, \n",
    "                    target_name, results_combined, results_start, results_end):\n",
    "    for i in range(2):\n",
    "        if i == 0:\n",
    "            name = name_start + ' Boosting'\n",
    "        else:\n",
    "            name = name_start +  ' Boosting (errors)'\n",
    "        results, results_test, mse_train, mse_test, mod1, pipeline1 = train_model_combination(df, dict_lag, model_start, model_middle, model_end, number_train, target_name, name, model_simple=i)\n",
    "        \n",
    "        results_new = pd.DataFrame([[name, name_start, i, mse_train, mse_test, mod1, pipeline1]], columns=columns_results)\n",
    "        # Save MSE \n",
    "        results_combined = pd.concat([results_combined, results_new])\n",
    "        results_combined = results_combined.reset_index(drop=True)\n",
    "\n",
    "        results_FI_start, results_FI_end = feature_importance(mod1, pipeline1, df, dict_lag, number_train, target_name)\n",
    "        results_FI_start.loc[0,'Model'] = name\n",
    "        results_FI_end.loc[0,'Model'] = name\n",
    "        results_start = pd.concat([results_start, results_FI_start])\n",
    "        results_end = pd.concat([results_end, results_FI_end])\n",
    "        results_start = results_start.reset_index(drop=True)\n",
    "        results_end = results_end.reset_index(drop=True)\n",
    "    return results_combined, results_start, results_end "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3912012",
   "metadata": {},
   "source": [
    "## Cacao elasticity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb3acb36",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_name, number_train, shares, dict_lag = load_data('cacao')\n",
    "results_start_cacao = pd.DataFrame([], columns=['Model'] + list(dict_lag.keys()))\n",
    "\n",
    "features_train_transf, features_test_transf, target_train, target_test, columns = preprocessing(df, dict_lag, \n",
    "                                                                                                number_train, target_name)\n",
    "columns_FE = features_test_transf.columns.values.tolist()\n",
    "results_end_cacao = pd.DataFrame([], columns=['Model'] + columns_FE)\n",
    "\n",
    "columns_results = ['Model_name', 'Model_typ', 'Variant', 'MSE Train', 'MSE Test', 'Model', 'Pipeline']\n",
    "results_combined_cacao = pd.DataFrame(columns = columns_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c27bbae",
   "metadata": {},
   "outputs": [],
   "source": [
    "corr = df.loc[:,list(dict_lag)].corr()\n",
    "fig_heatmap, ax_heatmap = plt.subplots(nrows=1, ncols=1, figsize=(10,10));\n",
    "sns.heatmap(corr.abs(), xticklabels=True, yticklabels=True, ax=ax_heatmap)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffc7accd",
   "metadata": {},
   "source": [
    "## Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad2e78ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_start = LinearRegression()\n",
    "model_middle = LinearRegression()\n",
    "model_end = LinearRegression()\n",
    "\n",
    "name = 'Linear Regression'\n",
    "results_combined_cacao, results_start_cacao, results_end_cacao = run_both_models(name, df, dict_lag, model_start, model_middle, model_end, number_train, target_name, results_combined_cacao, results_start_cacao, results_end_cacao)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68fd4374",
   "metadata": {},
   "source": [
    "## LinearSVR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01c72927",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_start = LinearSVR()\n",
    "model_middle = LinearSVR()\n",
    "model_end = LinearSVR()\n",
    "\n",
    "name = 'Linear SVR'\n",
    "results_combined_cacao, results_start_cacao, results_end_cacao = run_both_models(name, df, dict_lag, model_start, model_middle, model_end, number_train, target_name, results_combined_cacao, results_start_cacao, results_end_cacao)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58e7728b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_start = SVR()\n",
    "model_middle = SVR()\n",
    "model_end = SVR()\n",
    "\n",
    "name = 'SVR'\n",
    "results_combined_cacao, results_start_cacao, results_end_cacao = run_both_models(name, df, dict_lag, model_start, model_middle, model_end, number_train, target_name, results_combined_cacao, results_start_cacao, results_end_cacao)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c667c385",
   "metadata": {},
   "source": [
    "## Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0db8b08a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_start = RandomForestRegressor(n_estimators=5, max_depth=5)\n",
    "model_middle = RandomForestRegressor(n_estimators=5, max_depth=5)\n",
    "model_end = RandomForestRegressor(n_estimators=1, max_depth=5)\n",
    "\n",
    "name = 'Random Forest Regressor'\n",
    "results_combined_cacao, results_start_cacao, results_end_cacao = run_both_models(name, df, dict_lag, model_start, model_middle, model_end, number_train, target_name, results_combined_cacao, results_start_cacao, results_end_cacao)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "926cb3ef",
   "metadata": {},
   "source": [
    "### ANN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fde44b1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_name, number_train, shares, dict_lag = load_data('cacao', incl_shares=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5749c616",
   "metadata": {},
   "outputs": [],
   "source": [
    "shares, columns, target_train, target_test, features_train, features_test = preprocessing_split(df, dict_lag, number_train, target_name)\n",
    "ohe_tranf = ColumnTransformer(transformers = [('ohe', OneHotEncoder(sparse_output=False, handle_unknown='ignore'), \n",
    "                                              ['month'])], \n",
    "                              remainder = 'passthrough', verbose_feature_names_out=False)\n",
    "pipeline_ann = Pipeline([('DaIm', DataImputation(shares)), \n",
    "                               ('FE', FeatureEngineer(dict_lag)),\n",
    "                               ('ohe', ohe_tranf),\n",
    "                               ('sca', StandardScaler())])\n",
    "    \n",
    "features_train_transf = pipeline_ann.fit_transform(features_train)\n",
    "features_test_transf = pipeline_ann.transform(features_test)\n",
    "# make the features into a dataframe again\n",
    "features_train_transf = pd.DataFrame(columns=pipeline_ann.get_feature_names_out(), data=features_train_transf)\n",
    "features_train_transf.index=features_train.index\n",
    "features_test_transf = pd.DataFrame(columns=pipeline_ann.get_feature_names_out(), data=features_test_transf)\n",
    "features_test_transf.index=features_test.index\n",
    "columns = [x for x in pipeline_ann.get_feature_names_out() if x not in shares]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6d374f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_an = Sequential()\n",
    "model_an.add(Dense(50, activation=\"relu\", input_shape=(features_train_transf.shape[1],)))\n",
    "#model_an.add(Dense(500, activation = 'relu'))\n",
    "model_an.add(Dense(20, activation = 'relu'))\n",
    "#model_an.add(Dropout(rate=0.3))\n",
    "model_an.add(Dense(10, activation = 'relu'))\n",
    "#model_an.add(Dropout(rate=0.3))\n",
    "#model_an.add(Dense(10, activation = 'relu'))\n",
    "#model_an.add(Dropout(rate=0.3))\n",
    "model_an.add(Dense(1))\n",
    "\n",
    "model_an.compile(loss=tf.keras.losses.mse,\n",
    "                optimizer='adam',\n",
    "                #metrics = [\"mae\"]\n",
    "                )  # compile the model\n",
    "\n",
    "early_stop = EarlyStopping(monitor='val_loss', min_delta=0.001, patience=10)\n",
    "hist = model_an.fit(features_train_transf, \n",
    "                    target_train, \n",
    "                    validation_data=(features_test_transf, target_test),\n",
    "                    epochs=25, \n",
    "                    batch_size=1,\n",
    "                    callbacks=[early_stop]\n",
    "                   )#fits the model\n",
    "# batch_size: 32 bis 512, also 32, 64, 128, 256, 512\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots(nrows=1, ncols=2, figsize=(14, 5.5))\n",
    "l = len(hist.history['loss'])\n",
    "ax[0].plot(range(l), hist.history['loss'], linestyle=':', color='orange', label='Loss')\n",
    "ax[0].plot(range(l), hist.history['val_loss'], linestyle=':', color='green', label='validation loss')\n",
    "ax[0].legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95d682fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predictions on train set\n",
    "results = model_an.predict(features_train_transf)\n",
    "mse_train = mean_squared_error(results, target_train)\n",
    "print('MSE on Train set: ', mse_train)\n",
    "# Plot predictions on train set\n",
    "fig, ax = plt.subplots(nrows=1, ncols=1, figsize=size_diagrams)\n",
    "color_target = 'darkred'\n",
    "color_ann = 'darkgreen'\n",
    "ax.plot(target_train.index, results, label='ANN', color=color_ann);\n",
    "ax.plot(target_train.index, target_train,  label='Target', color=color_target);\n",
    "#Predictions on test set\n",
    "results_test = model_an.predict(features_test_transf)\n",
    "mse_test = mean_squared_error(results_test, target_test)\n",
    "print('MSE on Test set: ', mse_test)\n",
    "# Save MSE \n",
    "results_new = pd.DataFrame([['ANN', 'ANN', -1, mse_train, mse_test, model_an, pipeline_ann]], columns=columns_results)\n",
    "results_combined_cacao = pd.concat([results_combined_cacao, results_new])\n",
    "results_combined_cacao = results_combined_cacao.reset_index(drop=True)\n",
    "# Plot preditions on test set\n",
    "value = results[-1]\n",
    "value = value.tolist()[0]\n",
    "results_list = results_test.tolist()\n",
    "results_list.insert(0,[value])\n",
    "\n",
    "value_date = target_train.index[-1]\n",
    "new_index = target_test.index\n",
    "new_index= new_index.tolist()\n",
    "new_index.insert(0,value_date)\n",
    "ax.plot(new_index, results_list , color=color_ann, linestyle='--');\n",
    "ax.plot(new_index, [target_train.values.tolist()[-1]] + target_test.values.tolist(), color=color_target, linestyle='--');\n",
    "xlim = ax.get_xlim()\n",
    "ylim = ax.get_ylim()\n",
    "ylim_min = ax.get_ylim()[0]\n",
    "ylim_max = ax.get_ylim()[1]\n",
    "laenge = ylim_max - ylim_min \n",
    "laenge_neu = laenge* 0.9 \n",
    "laenge_neu2 = laenge_neu \n",
    "xmax = ax.get_xlim()[1]\n",
    "ax.fill_betweenx([laenge_neu2 * ylim_min / laenge , laenge_neu2 * ylim_max / laenge], pd.to_datetime('2025-02-28'), xmax, alpha=0.1, color='black', hatch='xx', label='Train data')\n",
    "ax.set_xlim([pd.to_datetime('2019-01-01'), pd.to_datetime('2026-01-01') ])\n",
    "ax.set_ylabel('Price elasticity', alpha=0.8)\n",
    "ax.set_title('ANN')\n",
    "ax.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48bd444a",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_FI_start, results_FI_end = feature_importance(model_an, pipeline_ann, df, dict_lag,number_train, target_name, predict_typ='ann')\n",
    "results_FI_start.loc[0, 'Model'] = 'ANN'\n",
    "results_FI_end.loc[0, 'Model'] = 'ANN'\n",
    "results_start_cacao  = pd.concat([results_start_cacao, results_FI_start])\n",
    "results_end_cacao = pd.concat([results_end_cacao, results_FI_end])\n",
    "results_start_cacao = results_start_cacao.reset_index(drop=True)\n",
    "results_end_cacao = results_end_cacao.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd778ef8",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_combined_cacao"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b7ed274",
   "metadata": {},
   "source": [
    "## Coffee elasticity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4affeb95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize values\n",
    "target_name, number_train, shares, dict_lag = load_data('coffee')\n",
    "results_start = pd.DataFrame([], columns=['Model'] + list(dict_lag.keys()))\n",
    "\n",
    "features_train_transf, features_test_transf, target_train, target_test, columns = preprocessing(df, dict_lag, \n",
    "                                                                                                number_train, target_name)\n",
    "columns_FE = features_test_transf.columns.values.tolist()\n",
    "results_end = pd.DataFrame([], columns=['Model'] + columns_FE)\n",
    "\n",
    "columns_results = ['Model_name', 'Model_typ', 'Variant', 'MSE Train', 'MSE Test', 'Model', 'Pipeline']\n",
    "results_combined = pd.DataFrame(columns = columns_results)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc1082b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_reduced = df.loc[:,list(dict_lag)]\n",
    "myFE = FeatureEngineer(dict_lag)\n",
    "    \n",
    "df_en = myFE.fit_transform(df_reduced)\n",
    "corr = df_en.corr()\n",
    "fig_heatmap, ax_heatmap = plt.subplots(nrows=1, ncols=1, figsize=(10,10));\n",
    "sns.heatmap(corr.abs(), xticklabels=True, yticklabels=True, ax=ax_heatmap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "484eded3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_en = df.loc[:,['EinfPr_Kaffee und Tee, Kaffee-Ersatz', 'ErzPr_Kaffee und Tee, Kaffee-Ersatz', \n",
    "                   'VPI_Kaffee und Ähnliches', 'Ferrero', 'LindtSpruengli', 'Mondelez', \n",
    "                   'PCOCOUSDM', 'PCOFFROBUSDM', 'elasticity_coffee']]\n",
    "corr = df_en.corr()\n",
    "fig_heatmap, ax_heatmap = plt.subplots(nrows=1, ncols=1, figsize=(10,10));\n",
    "sns.heatmap(corr.abs(), xticklabels=True, yticklabels=True, ax=ax_heatmap)\n",
    "sns.set(font_scale=4)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94057168",
   "metadata": {},
   "source": [
    "### Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17e9e05a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_start = LinearRegression()\n",
    "model_middle = LinearRegression()\n",
    "model_end = LinearRegression()\n",
    "\n",
    "name = 'Linear Regression'\n",
    "results_combined, results_start, results_end = run_both_models(name, df, dict_lag, model_start, model_middle, model_end, number_train, target_name, results_combined, results_start, results_end)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93be9699",
   "metadata": {},
   "source": [
    "### Linear SVR\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67390dbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_start = LinearSVR()\n",
    "model_middle = LinearSVR()\n",
    "model_end = LinearSVR()\n",
    "\n",
    "name = 'Linear SVR'\n",
    "results_combined, results_start, results_end = run_both_models(name, df, dict_lag, model_start, model_middle, model_end, number_train, target_name, results_combined, results_start, results_end)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54f8f1d4",
   "metadata": {},
   "source": [
    "### SVR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e660a683",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_start = SVR()\n",
    "model_middle = SVR()\n",
    "model_end = SVR()\n",
    "\n",
    "name = 'SVR'\n",
    "results_combined, results_start, results_end = run_both_models(name, df, dict_lag, model_start, model_middle, model_end, number_train, target_name, results_combined, results_start, results_end)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df087f15",
   "metadata": {},
   "source": [
    "### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc02f082",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grid search for n_estimators in random forest \n",
    "for n_1 in range(1,10):\n",
    "    for n_2 in range(1,10):\n",
    "        for n_3 in range(1,10):\n",
    "\n",
    "\n",
    "            model_start = RandomForestRegressor(n_estimators=n_1, max_depth=5, random_state=42)\n",
    "            model_middle = RandomForestRegressor(n_estimators=n_2, max_depth=5, random_state=42)\n",
    "            model_end = RandomForestRegressor(n_estimators=n_3, max_depth=5, random_state=42)\n",
    "\n",
    "            name = 'Random Forest Regressor Boosting' + str(n_1) + str(n_2) + str(n_3)\n",
    "            results_combined = run_both_models(name, df, dict_lag, model_start, model_middle, model_end, number_train, target_name, results_combined)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "950eb2dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_combined.sort_values(['MSE Test']).head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c571d1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_start = RandomForestRegressor(n_estimators=1, max_depth=5, random_state=42)\n",
    "model_middle = RandomForestRegressor(n_estimators=3, max_depth=5, random_state=42)\n",
    "model_end = RandomForestRegressor(n_estimators=8, max_depth=5, random_state=42)\n",
    "\n",
    "name = 'Random Forest Regressor' \n",
    "results_combined, results_start, results_end = run_both_models(name, df, dict_lag, model_start, model_middle, model_end, number_train, target_name, results_combined, results_start, results_end)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "733164d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_start = RandomForestRegressor(n_estimators=8, max_depth=5, random_state=42)\n",
    "model_middle = RandomForestRegressor(n_estimators=5, max_depth=5, random_state=42)\n",
    "model_end = RandomForestRegressor(n_estimators=1, max_depth=5, random_state=42)\n",
    "\n",
    "name = 'Random Forest Regressor' \n",
    "results_combined, results_start, results_end = run_both_models(name, df, dict_lag, model_start, model_middle, model_end, number_train, target_name, results_combined, results_start, results_end)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59793e2d",
   "metadata": {},
   "source": [
    "### ANN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d84c84c",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_name, number_train, shares, dict_lag = load_data('coffee', incl_shares=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "657eb1f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "shares, columns, target_train, target_test, features_train, features_test = preprocessing_split(df, dict_lag, number_train, target_name)\n",
    "ohe_tranf = ColumnTransformer(transformers = [('ohe', OneHotEncoder(sparse_output=False, handle_unknown='ignore'), \n",
    "                                              ['month'])], \n",
    "                              remainder = 'passthrough', verbose_feature_names_out=False)\n",
    "pipeline_ann = Pipeline([('DaIm', DataImputation(shares)), \n",
    "                               ('FE', FeatureEngineer(dict_lag)),\n",
    "                               ('ohe', ohe_tranf),\n",
    "                               ('sca', StandardScaler())])\n",
    "    \n",
    "features_train_transf = pipeline_ann.fit_transform(features_train)\n",
    "features_test_transf = pipeline_ann.transform(features_test)\n",
    "# make the features into a dataframe again\n",
    "features_train_transf = pd.DataFrame(columns=pipeline_ann.get_feature_names_out(), data=features_train_transf)\n",
    "features_train_transf.index=features_train.index\n",
    "features_test_transf = pd.DataFrame(columns=pipeline_ann.get_feature_names_out(), data=features_test_transf)\n",
    "features_test_transf.index=features_test.index\n",
    "columns = [x for x in pipeline_ann.get_feature_names_out() if x not in shares]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86fd4a2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_an = Sequential()\n",
    "model_an.add(Dense(50, activation=\"relu\", input_shape=(features_train_transf.shape[1],)))\n",
    "#model_an.add(Dense(500, activation = 'relu'))\n",
    "model_an.add(Dense(20, activation = 'relu'))\n",
    "#model_an.add(Dropout(rate=0.3))\n",
    "model_an.add(Dense(10, activation = 'relu'))\n",
    "#model_an.add(Dropout(rate=0.3))\n",
    "#model_an.add(Dense(10, activation = 'relu'))\n",
    "#model_an.add(Dropout(rate=0.3))\n",
    "model_an.add(Dense(1))\n",
    "\n",
    "model_an.compile(loss=tf.keras.losses.mse,\n",
    "                optimizer='adam',\n",
    "                #metrics = [\"mae\"]\n",
    "                )  # compile the model\n",
    "\n",
    "early_stop = EarlyStopping(monitor='val_loss', min_delta=0.001, patience=10)\n",
    "hist = model_an.fit(features_train_transf, \n",
    "                    target_train, \n",
    "                    validation_data=(features_test_transf, target_test),\n",
    "                    epochs=25, \n",
    "                    batch_size=1,\n",
    "                    callbacks=[early_stop]\n",
    "                   )#fits the model\n",
    "# batch_size: 32 bis 512, also 32, 64, 128, 256, 512\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots(nrows=1, ncols=2, figsize=(14, 5.5))\n",
    "l = len(hist.history['loss'])\n",
    "ax[0].plot(range(l), hist.history['loss'], linestyle=':', color='orange', label='Loss')\n",
    "ax[0].plot(range(l), hist.history['val_loss'], linestyle=':', color='green', label='validation loss')\n",
    "ax[0].legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70aae522",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predictions on train set\n",
    "results = model_an.predict(features_train_transf)\n",
    "mse_train = mean_squared_error(results, target_train)\n",
    "print('MSE on Train set: ', mse_train)\n",
    "# Plot predictions on train set\n",
    "fig, ax = plt.subplots(nrows=1, ncols=1, figsize=size_diagrams)\n",
    "color_target = 'darkred'\n",
    "color_ann = 'darkgreen'\n",
    "ax.plot(target_train.index, results, label='ANN', color=color_ann);\n",
    "ax.plot(target_train.index, target_train,  label='Target', color=color_target);\n",
    "#Predictions on test set\n",
    "results_test = model_an.predict(features_test_transf)\n",
    "mse_test = mean_squared_error(results_test, target_test)\n",
    "print('MSE on Test set: ', mse_test)\n",
    "# Save MSE \n",
    "results_new = pd.DataFrame([['ANN', 'ANN', -1, mse_train, mse_test, model_an, pipeline_ann]], columns=columns_results)\n",
    "results_combined = pd.concat([results_combined, results_new])\n",
    "results_combined = results_combined.reset_index(drop=True)\n",
    "# Plot preditions on test set\n",
    "value = results[-1]\n",
    "value = value.tolist()[0]\n",
    "results_list = results_test.tolist()\n",
    "results_list.insert(0,[value])\n",
    "\n",
    "value_date = target_train.index[-1]\n",
    "new_index = target_test.index\n",
    "new_index= new_index.tolist()\n",
    "new_index.insert(0,value_date)\n",
    "ax.plot(new_index, results_list , color=color_ann, linestyle='--');\n",
    "ax.plot(new_index, [target_train.values.tolist()[-1]] + target_test.values.tolist(), color=color_target, linestyle='--');\n",
    "xlim = ax.get_xlim()\n",
    "ylim = ax.get_ylim()\n",
    "ylim_min = ax.get_ylim()[0]\n",
    "ylim_max = ax.get_ylim()[1]\n",
    "laenge = ylim_max - ylim_min \n",
    "laenge_neu = laenge* 0.9 \n",
    "laenge_neu2 = laenge_neu \n",
    "xmax = ax.get_xlim()[1]\n",
    "ax.fill_betweenx([laenge_neu2 * ylim_min / laenge , laenge_neu2 * ylim_max / laenge], pd.to_datetime('2025-02-28'), xmax, alpha=0.1, color='black', hatch='xx', label='Train data')\n",
    "ax.set_xlim([pd.to_datetime('2019-01-01'), pd.to_datetime('2026-01-01') ])\n",
    "ax.set_ylabel('Price elasticity', alpha=0.8)\n",
    "ax.set_title('ANN')\n",
    "ax.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d39644c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_FI_start, results_FI_end = feature_importance(model_an, pipeline_ann, df, dict_lag,number_train, target_name, predict_typ='ann')\n",
    "results_FI_start.loc[0, 'Model'] = 'ANN'\n",
    "results_FI_end.loc[0, 'Model'] = 'ANN'\n",
    "results_start = pd.concat([results_start, results_FI_start])\n",
    "results_end = pd.concat([results_end, results_FI_end])\n",
    "results_start = results_start.reset_index(drop=True)\n",
    "results_end = results_end.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3ecdf42",
   "metadata": {},
   "source": [
    "### ARIMAX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ca82e98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ignore filtewarnings\n",
    "warnings.filterwarnings('once')\n",
    "warnings.filterwarnings('ignore', category=DeprecationWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee91fdf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_name, number_train, shares, dict_lag = load_data('coffee', incl_shares = False)\n",
    "\n",
    "shares, columns, target_train, target_test, features_train, features_test  = preprocessing_split(df, dict_lag, number_train, target_name)\n",
    "ohe_tranf = ColumnTransformer(transformers = [('ohe', OneHotEncoder(sparse_output=False, handle_unknown='ignore'), \n",
    "                                               ['month'])], \n",
    "                              remainder = 'passthrough', verbose_feature_names_out=False)\n",
    "pipeline_arimax = Pipeline([('DaIm', DataImputation(shares)), \n",
    "                                  ('FE', FeatureEngineer(dict_lag)),\n",
    "                                  ('ohe', ohe_tranf),\n",
    "                                  ('sca', StandardScaler())])\n",
    "    \n",
    "features_train_transf = pipeline_arimax.fit_transform(features_train)\n",
    "features_test_transf = pipeline_arimax.transform(features_test)\n",
    "# make the features into a dataframe again\n",
    "features_train_transf = pd.DataFrame(columns=pipeline_arimax.get_feature_names_out(), data=features_train_transf)\n",
    "features_train_transf.index = features_train.index\n",
    "features_test_transf = pd.DataFrame(columns=pipeline_arimax.get_feature_names_out(), data=features_test_transf)\n",
    "features_test_transf.index = features_test.index\n",
    "columns = [x for x in pipeline_arimax.get_feature_names_out() if x not in shares]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4443c557",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train-test dates\n",
    "# ==============================================================================\n",
    "end_train = '2025-03-31'\n",
    "print(\n",
    "    f\"Train dates : {data.index.min()} --- {data.loc[:end_train].index.max()}  \"\n",
    "    f\"(n={len(data.loc[:end_train])})\"\n",
    ")\n",
    "print(\n",
    "    f\"Test dates  : {data.loc[end_train:].index.min()} --- {data.loc[:].index.max()}  \"\n",
    "    f\"(n={len(data.loc[end_train:])})\"\n",
    ")\n",
    "data_train = data.loc[:end_train]\n",
    "data_test  = data.loc[end_train:]\n",
    "df_train = df.loc[:end_train, :]\n",
    "df_test = df.loc[end_train : , :]\n",
    "features_train_transf, \n",
    "features_test_transf, \n",
    "target_train, \n",
    "target_test\n",
    "# Plot\n",
    "# ==============================================================================\n",
    "#set_dark_theme()\n",
    "fig, ax=plt.subplots(figsize=(7, 3))\n",
    "ax.plot(data_train, label='train')\n",
    "ax.plot(data_test, label='test')\n",
    "ax.set_title('Coffee price elasticity')\n",
    "ax.legend();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a8437e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_diff_1 = data_train.diff().dropna()\n",
    "data_diff_2 = data_diff_1.diff().dropna()\n",
    "\n",
    "# Suppress warnings for stationarity tests\n",
    "with warnings.catch_warnings():\n",
    "    warnings.filterwarnings('ignore')\n",
    "    print('Test stationarity for original series')\n",
    "    print('-------------------------------------')\n",
    "    adfuller_result = adfuller(data)\n",
    "    kpss_result = kpss(data)\n",
    "    print(f'ADF Statistic: {adfuller_result[0]}, p-value: {adfuller_result[1]}')\n",
    "    print(f'KPSS Statistic: {kpss_result[0]}, p-value: {kpss_result[1]}')\n",
    "\n",
    "    print('\\nTest stationarity for differenced series (order=1)')\n",
    "    print('--------------------------------------------------')\n",
    "    adfuller_result = adfuller(data_diff_1)\n",
    "    kpss_result = kpss(data.diff().dropna())\n",
    "    print(f'ADF Statistic: {adfuller_result[0]}, p-value: {adfuller_result[1]}')\n",
    "    print(f'KPSS Statistic: {kpss_result[0]}, p-value: {kpss_result[1]}')\n",
    "\n",
    "    print('\\nTest stationarity for differenced series (order=2)')\n",
    "    print('--------------------------------------------------')\n",
    "    adfuller_result = adfuller(data_diff_2)\n",
    "    kpss_result = kpss(data.diff().diff().dropna())\n",
    "    print(f'ADF Statistic: {adfuller_result[0]}, p-value: {adfuller_result[1]}')\n",
    "    print(f'KPSS Statistic: {kpss_result[0]}, p-value: {kpss_result[1]}')\n",
    "\n",
    "# Plot series\n",
    "# ==============================================================================\n",
    "fig, axs = plt.subplots(nrows=3, ncols=1, figsize=(7, 5), sharex=True)\n",
    "data.plot(ax=axs[0], title='Original time series')\n",
    "data_diff_1.plot(ax=axs[1], title='Differenced order 1')\n",
    "data_diff_2.plot(ax=axs[2], title='Differenced order 2');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc973518",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Autocorrelation plot for original and differentiated series\n",
    "# ==============================================================================\n",
    "fig, axs = plt.subplots(nrows=2, ncols=1, figsize=(6, 4), sharex=True)\n",
    "plot_acf(data, ax=axs[0], lags=50, alpha=0.05)\n",
    "axs[0].set_title('Autocorrelation original series')\n",
    "plot_acf(data_diff_1, ax=axs[1], lags=50, alpha=0.05)\n",
    "axs[1].set_title('Autocorrelation differentiated series (order=1)');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d50496ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Partial autocorrelation plot for original and differenced series\n",
    "# ==============================================================================\n",
    "fig, axs = plt.subplots(nrows=2, ncols=1, figsize=(6, 3), sharex=True)\n",
    "plot_pacf(data, ax=axs[0], lags=40, alpha=0.05)\n",
    "axs[0].set_title('Partial autocorrelation original series')\n",
    "plot_pacf(data_diff_1, ax=axs[1], lags=36, alpha=0.05)\n",
    "axs[1].set_title('Partial autocorrelation differenced series (order=1)');\n",
    "plt.tight_layout();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f69477c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Time series decomposition of original versus differenced series\n",
    "# ==============================================================================\n",
    "res_decompose = seasonal_decompose(data, model='additive', extrapolate_trend='freq', period=12)\n",
    "res_descompose_diff_2 = seasonal_decompose(data_diff_1, model='additive', extrapolate_trend='freq', period=12)\n",
    "\n",
    "fig, axs = plt.subplots(nrows=4, ncols=2, figsize=(9, 6), sharex=True)\n",
    "\n",
    "res_decompose.observed.plot(ax=axs[0, 0])\n",
    "axs[0, 0].set_title('Original series', fontsize=12)\n",
    "res_decompose.trend.plot(ax=axs[1, 0])\n",
    "axs[1, 0].set_title('Trend', fontsize=12)\n",
    "res_decompose.seasonal.plot(ax=axs[2, 0])\n",
    "axs[2, 0].set_title('Seasonal', fontsize=12)\n",
    "res_decompose.resid.plot(ax=axs[3, 0])\n",
    "axs[3, 0].set_title('Residuals', fontsize=12)\n",
    "res_descompose_diff_2.observed.plot(ax=axs[0, 1])\n",
    "axs[0, 1].set_title('Differenced series (order=1)', fontsize=12)\n",
    "res_descompose_diff_2.trend.plot(ax=axs[1, 1])\n",
    "axs[1, 1].set_title('Trend', fontsize=12)\n",
    "res_descompose_diff_2.seasonal.plot(ax=axs[2, 1])\n",
    "axs[2, 1].set_title('Seasonal', fontsize=12)\n",
    "res_descompose_diff_2.resid.plot(ax=axs[3, 1])\n",
    "axs[3, 1].set_title('Residuals', fontsize=12)\n",
    "fig.suptitle('Time series decomposition original series versus differenced series', fontsize=14)\n",
    "fig.tight_layout();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63837092",
   "metadata": {},
   "source": [
    "### statsmodels.Sarimax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a23d06a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ARIMA model with statsmodels.Sarimax\n",
    "# ==============================================================================\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning, message='Non-invertible|Non-stationary')\n",
    "model = SARIMAX(endog = target_train, exog = features_train_transf, order = (1, 1, 1), seasonal_order = (1, 1, 1, 12))\n",
    "model_res = model.fit(disp=0)\n",
    "model_res.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ac8e17f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prediction\n",
    "# ==============================================================================\n",
    "predictions_statsmodels = model_res.get_forecast(steps=len(target_test), exog = features_test_transf).predicted_mean\n",
    "predictions_statsmodels.name = 'predictions_statsmodels'\n",
    "predictions_statsmodels.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d44ac767",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_FI_start, results_FI_end = feature_importance(model_res, pipeline_arimax, df, dict_lag, number_train, target_name, predict_typ='stat')\n",
    "\n",
    "results_FI_start.loc[0, 'Model'] = 'statsmodel'\n",
    "results_FI_end.loc[0, 'Model'] = 'statsmodel'\n",
    "results_start = pd.concat([results_start, results_FI_start])\n",
    "results_end = pd.concat([results_end, results_FI_end])\n",
    "results_start = results_start.reset_index(drop=True)\n",
    "results_end = results_end.reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8114c355",
   "metadata": {},
   "source": [
    "### skforecast Sarimax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0b4c018",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ARIMA model with skforecast Sarimax\n",
    "# ==============================================================================\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning, message='Non-invertible|Non-stationary')\n",
    "model = Sarimax(order=(1, 1, 1), seasonal_order=(1, 1, 1, 12))\n",
    "model.fit(y=target_train, exog = features_train_transf)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54974186",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prediction\n",
    "# ==============================================================================\n",
    "predictions_skforecast_sarimax = model.predict(steps=len(target_test), exog = features_test_transf)\n",
    "predictions_skforecast_sarimax = pd.DataFrame(predictions_skforecast_sarimax, index=target_test.index)\n",
    "predictions_skforecast_sarimax.columns = ['skforecast']\n",
    "display(predictions_skforecast_sarimax.head(4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "647524d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_FI_start, results_FI_end = feature_importance(model, pipeline_arimax, df, dict_lag, number_train, target_name, predict_typ='stat')\n",
    "\n",
    "results_FI_start.loc[0, 'Model'] = 'sarimax'\n",
    "results_FI_end.loc[0, 'Model'] = 'sarimax'\n",
    "results_start = pd.concat([results_start, results_FI_start])\n",
    "results_end = pd.concat([results_end, results_FI_end])\n",
    "results_start = results_start.reset_index(drop=True)\n",
    "results_end = results_end.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9d0086a",
   "metadata": {},
   "source": [
    "### skforecast ARIMA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6cc6383",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ARIMA model with skforecast Arima\n",
    "# ==============================================================================\n",
    "model = Arima(order=(1, 1, 1), seasonal_order=(1, 1, 1), m=12)\n",
    "model.fit(y=target_train,  suppress_warnings=True)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a6c4227",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prediction\n",
    "# ==============================================================================\n",
    "predictions_skforecast_arima = model.predict(steps=len(target_test))\n",
    "#pred_index = expand_index(index=data_train.index, steps=len(data_test))\n",
    "predictions_skforecast_arima = pd.Series(predictions_skforecast_arima, index=target_test.index)\n",
    "predictions_skforecast_arima.head(4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b818ea5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_FI_start, results_FI_end = feature_importance(model, pipeline_arimax, df, dict_lag, number_train, target_name, predict_typ='stat')\n",
    "\n",
    "results_FI_start.loc[0, 'Model'] = 'arima'\n",
    "results_FI_end.loc[0, 'Model'] = 'arimax'\n",
    "results_start = pd.concat([results_start, results_FI_start])\n",
    "results_end = pd.concat([results_end, results_FI_end])\n",
    "results_start = results_start.reset_index(drop=True)\n",
    "results_end = results_end.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae95f30e",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_start"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e6013bd",
   "metadata": {},
   "source": [
    "## Comparison of methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb22efb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot predictions\n",
    "# ==============================================================================\n",
    "fig, ax = plt.subplots(figsize=size_diagrams)\n",
    "color_target = 'darkred'\n",
    "color_final = 'darkgreen'\n",
    "color_m = 'orchid'\n",
    "color_mf = 'lightblue'\n",
    "color_mfl = 'pink'\n",
    "\n",
    "value_date = target_train.index[-1]\n",
    "new_index = target_test.index\n",
    "new_index= new_index.tolist()\n",
    "new_index.insert(0,value_date)\n",
    "\n",
    "value = target_train[-1]\n",
    "labels = ['statsmodels', 'skforecast sarimax', 'skforecast arima']\n",
    "colors = ['darkgreen', 'darkblue', 'teal']\n",
    "predictions = [predictions_statsmodels, predictions_skforecast_sarimax, predictions_skforecast_arima]\n",
    "for i in range(3):\n",
    "    pred = predictions[i]\n",
    "    results_list = pred.values.tolist()\n",
    "    if i in [0,2]:\n",
    "        results_list.insert(0, value)\n",
    "    else: \n",
    "        results_list.insert(0, [value])\n",
    "    print(results_list)\n",
    "    ax.plot(new_index, results_list, label=labels[i], color=colors[i], linestyle='--')\n",
    "ax.plot(new_index, [target_train.values.tolist()[-1]] + target_test.values.tolist(), color=color_target, linestyle='--');\n",
    "ax.plot(target_train, label='Target', color=color_target )\n",
    "xlim = ax.get_xlim()\n",
    "ylim = ax.get_ylim()\n",
    "ylim_min = ax.get_ylim()[0]\n",
    "ylim_max = ax.get_ylim()[1]\n",
    "laenge = ylim_max - ylim_min \n",
    "laenge_neu = laenge* 0.9 \n",
    "laenge_neu2 = laenge_neu \n",
    "xmax = ax.get_xlim()[1]\n",
    "ax.fill_betweenx([laenge_neu2 * ylim_min / laenge , laenge_neu2 * ylim_max / laenge], pd.to_datetime('2025-02-28'), xmax, alpha=0.1, color='black', hatch='xx', label='Train data')\n",
    "ax.set_xlim([pd.to_datetime('2019-01-01'), pd.to_datetime('2026-01-01') ])\n",
    "ax.set_ylabel('Price elasticity', alpha=0.8)\n",
    "ax.set_title('Predictions with SARIMAX models')\n",
    "ax.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82140084",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_new = pd.DataFrame([['Sarimax statsmodel', 'Sarimax', -1, 0, mean_squared_error(target_test, predictions_statsmodels.values), float('nan'), pipeline_arimax]], columns=columns_results)\n",
    "results_combined = pd.concat([results_combined, results_new])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0501b75e",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_new = pd.DataFrame([['Sarimax skforecast', 'Sarimax', -1, 0, mean_squared_error(target_test, predictions_skforecast_sarimax.skforecast.values), float('nan'), pipeline_arimax]], columns=columns_results)\n",
    "results_combined = pd.concat([results_combined, results_new])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ee2854c",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_new = pd.DataFrame([['arima skforecast', 'arima', -1, 0, mean_squared_error(target_test, predictions_skforecast_arima.values), float('nan'), pipeline_arimax]], columns=columns_results)\n",
    "results_combined = pd.concat([results_combined, results_new])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05c0a517",
   "metadata": {},
   "source": [
    "## Compare different models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "848f6b55",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_combined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b65620c",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(nrows=1, ncols=1, figsize=(5,5))\n",
    "for model in results_combined['Model_typ'].unique():\n",
    "    mask = results_combined['Model_typ'] == model\n",
    "    ax.scatter(results_combined.loc[mask, 'MSE Train'],results_combined.loc[mask,'MSE Test'], label = model)\n",
    "ylim = ax.get_ylim()\n",
    "xlim = ax.get_xlim()\n",
    "ax.plot(range(20), range(20), alpha = 0.2, color = 'darkred')\n",
    "ax.set_ylim(0, ylim[1])\n",
    "ax.set_xlim(0, xlim[1])\n",
    "ax.set_xlabel('MSE on train set')\n",
    "ax.set_ylabel('MSE on test set')\n",
    "ax.set_title('Mean squared errors on Train and Test sets')\n",
    "ax.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a889e7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_feature_importance(results_start, results_end):\n",
    "    # Plot feature importance\n",
    "    fig_FI, ax_FI = plt.subplots(nrows=2, ncols=1, figsize=(10,15))\n",
    "    for i in range(2):\n",
    "        results = [results_start, results_end][i]\n",
    "        barHeight = 0.9/results.shape[0]\n",
    "        br1 = np.arange(results.shape[1]-1)\n",
    "        list_br = [br1]\n",
    "        for model in range(results.shape[0]):\n",
    "            br_new = [x + barHeight for x in list_br[-1]] \n",
    "            list_br.append(br_new)\n",
    "            ax_FI[i].barh(y=br_new, height=barHeight, width=results.iloc[model, 1:], \n",
    "                        alpha=0.9,  tick_label=results.columns.values[1:], label = results.iloc[model, 0])    \n",
    "            ax_FI[i].spines['bottom'].set_visible(False)\n",
    "            ax_FI[i].spines['top'].set_visible(False)\n",
    "            ax_FI[i].spines['right'].set_visible(False)\n",
    "        ax_FI[i].legend()\n",
    "        if i  == 0:\n",
    "            ax_FI[i].set_title('Feature Importance of original features')\n",
    "        else:\n",
    "            ax_FI[i].set_title('Feature Importance of engineered features')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba76dfef",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ff358c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = (results_start['Model'] == 'Linear Regression Boosting') | (results_start['Model'] == 'ANN')| (results_start['Model'] == 'Linear SVR Boosting')\n",
    "\n",
    "\n",
    "results_start_selection = results_start.loc[mask,:]\n",
    "results_end_selection = results_end.loc[mask,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7b10705",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_feature_importance(results_start_selection, results_end_selection)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "chocolate-project (3.13.11)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
